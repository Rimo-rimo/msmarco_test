{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from pymilvus import model\n",
    "from pymilvus import MilvusClient, Collection, connections, DataType, CollectionSchema, FieldSchema\n",
    "import numpy as np\n",
    "import json\n",
    "from FlagEmbedding import FlagReranker\n",
    "from pymilvus.model.reranker import BGERerankFunction\n",
    "import random\n",
    "import subprocess\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset load\n",
    "data = pd.read_csv(\"../data/top1000_dev.tsv\", sep='\\t', names=['qid', 'pid', 'query', 'passage'])\n",
    "unique_query = pd.read_csv(\"../data/unique_query.csv\")\n",
    "qrels = pd.read_csv(\"../data/qrels.dev.small.tsv\", sep='\\t', names=['qid', 'r', 'pid', 'l'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/all_pid_list.pkl', 'rb') as file:\n",
    "    all_pid_list = pickle.load(file)\n",
    "with open('../data/train_pid_list.pkl', 'rb') as file:\n",
    "    train_pid_list = pickle.load(file)\n",
    "with open('../data/test_pid_list.pkl', 'rb') as file:\n",
    "    test_pid_list = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Keywords Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3fc24cd965b4c0ea8d6f8394fe99c45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 키워드 추출 모델 -> BAAI/bge-m3 활용한 키워드 추출\n",
    "kw_model = KeyBERT(\"BAAI/bge-m3\")\n",
    "\n",
    "# 유사도 검색을 위한 embedding 모델 \n",
    "bge_m3_ef = model.hybrid.BGEM3EmbeddingFunction(\n",
    "        model_name= \"BAAI/bge-m3\",\n",
    "        batch_size = 16,\n",
    "        device = \"cuda:1\",\n",
    "        # use_fp16 = True,\n",
    "        return_dense = True,\n",
    "        return_sparse = False,\n",
    "        return_colbert_vecs = False,\n",
    "    )\n",
    "\n",
    "# 벡터DB -> Milvus\n",
    "client = MilvusClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid_list = data[\"pid\"].tolist()\n",
    "\n",
    "# Easy Nagative 를 위한 랜덤 pid 선택 함수\n",
    "def get_random_pid(pid_list, exclude_pid, total_numbers):\n",
    "    pid_list_rerange = [pid for pid in pid_list if pid != exclude_pid]  # 제외할 숫자 제거\n",
    "    random_numbers = random.sample(pid_list_rerange, total_numbers) \n",
    "    return random_numbers\n",
    "\n",
    "# passage로 부터 qseudo query 추출 및 finetuning dataset 구성 함수\n",
    "def get_keyword_query(top_n, negative_type):\n",
    "\n",
    "    train_json = []\n",
    "\n",
    "    for pid in tqdm(train_pid_list):\n",
    "        passage = data[data[\"pid\"] == pid][\"passage\"].tolist()[0]\n",
    "\n",
    "        passage_keywords = kw_model.extract_keywords(passage, keyphrase_ngram_range=(1,1), top_n=top_n) # 키워드 추출\n",
    "        passage_keywords = sorted(passage_keywords, key=lambda x: passage.find(x[0])) # 추출된 키워드를 문장 내의 순서대로 정렬\n",
    "        query = \" \".join([i[0] for i in passage_keywords]) # 추출된 키워드를 하나의 string으로 결합\n",
    "\n",
    "        query_vectors = bge_m3_ef.encode_queries([query])[\"dense\"] # pseudo query에 대한 embedding\n",
    "\n",
    "        # hard negative를 위한 passage vector search\n",
    "        if negative_type == \"hard\":\n",
    "            res = client.search(\n",
    "                collection_name=\"msmarco_bgem3\",\n",
    "                data=query_vectors,\n",
    "                limit=10,\n",
    "                output_fields=[\"text\"],\n",
    "                anns_field=\"dense_vector\",\n",
    "                filter=f\"pid != {pid}\",\n",
    "            )\n",
    "            neg_list = [i[\"entity\"][\"text\"] for i in res[0]]\n",
    "            \n",
    "        # easy negative를 위한 passage 랜덤 선택\n",
    "        elif negative_type == \"easy\":\n",
    "             neg_pid = get_random_pid(pid_list, pid, 10)\n",
    "             neg_list = [data[data[\"pid\"] == i][\"passage\"].tolist()[0] for i in neg_pid]\n",
    "\n",
    "        # finetuning dataset format\n",
    "        train_json.append({\n",
    "            \"query\": query, \n",
    "            \"pos\": passage, \n",
    "            \"neg\": neg_list\n",
    "                })\n",
    "\n",
    "    return train_json\n",
    "\n",
    "# Ground Truth finetuning을 위한 데이터셋 구성 함수\n",
    "def get_gt_query(negative_type):\n",
    "    train_json = []\n",
    "    for pid in tqdm(train_pid_list):\n",
    "        passage = data[data[\"pid\"] == pid][\"passage\"].tolist()[0]\n",
    "        qid = qrels[qrels[\"pid\"] == pid][\"qid\"].tolist()[0]\n",
    "        query = unique_query[unique_query[\"qid\"] == qid][\"query\"].tolist()[0]\n",
    "\n",
    "        # hard negative를 위한 passage vector search\n",
    "        if negative_type == \"hard\":\n",
    "            query_vectors = bge_m3_ef.encode_queries([query])[\"dense\"]\n",
    "            res = client.search(\n",
    "                collection_name=\"msmarco_bgem3\",\n",
    "                data=query_vectors,\n",
    "                limit=10,\n",
    "                output_fields=[\"text\"],\n",
    "                anns_field=\"dense_vector\",\n",
    "                filter=f\"pid != {pid}\",\n",
    "            )\n",
    "            neg_list = [i[\"entity\"][\"text\"] for i in res[0]]\n",
    "\n",
    "        # easy negative를 위한 passage vector search\n",
    "        elif negative_type == \"easy\":\n",
    "             neg_pid = get_random_pid(pid_list, pid, 10)\n",
    "             neg_list = [data[data[\"pid\"] == i][\"passage\"].tolist()[0] for i in neg_pid]\n",
    "\n",
    "        # finetuning dataset format\n",
    "        train_json.append({\n",
    "            \"query\": query, \n",
    "            \"pos\": passage, \n",
    "            \"neg\": neg_list\n",
    "                })\n",
    "\n",
    "    return train_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_num_list = [3,5,7,9]\n",
    "negative_type_list = [\"hard\", \"easy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pseudo query dataset 생성\n",
    "for keyword_num in keyword_num_list:\n",
    "    for negative_type in negative_type_list:\n",
    "\n",
    "        file_path = f\"../data/finetuning_dataset/kw_{keyword_num}_{negative_type}.jsonl\"\n",
    "\n",
    "        # finetuning dataset 생성\n",
    "        result = get_keyword_query(keyword_num, negative_type)\n",
    "\n",
    "        # 저장\n",
    "        with open(file_path , encoding= \"utf-8\",mode=\"w\") as file: \n",
    "\n",
    "# gt query dataset 생성\n",
    "re = get_gt_query(\"easy\")\n",
    "with open(\"../data/finetuning_dataset/gt_easy_train.jsonl\" , encoding= \"utf-8\",mode=\"w\") as file: \n",
    "\tfor i in re: file.write(json.dumps(i) + \"\\n\")\n",
    "\n",
    "re = get_gt_query(\"hard\")\n",
    "with open(\"../data/finetuning_dataset/gt_hard_train.jsonl\" , encoding= \"utf-8\",mode=\"w\") as file: \n",
    "\tfor i in re: file.write(json.dumps(i) + \"\\n\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! torchrun --nproc_per_node 2 -m FlagEmbedding.reranker.run --output_dir ../model/kw_3_easy_train --model_name_or_path BAAI/bge-reranker-v2-m3 --train_data ../data/finetuning_dataset/kw_3_easy_train.jsonl --learning_rate 5e-6 --fp16 --num_train_epochs 40 --per_device_train_batch_size 2 --gradient_accumulation_steps 32 --dataloader_drop_last True --train_group_size 3 --max_len 512 --weight_decay 0.01 --logging_steps 10 --save_steps 100 ; torchrun --nproc_per_node 2 -m FlagEmbedding.reranker.run --output_dir ../model/gt_easy_train --model_name_or_path BAAI/bge-reranker-v2-m3 --train_data ../data/finetuning_dataset/gt_easy_train.jsonl --learning_rate 5e-6 --fp16 --num_train_epochs 40 --per_device_train_batch_size 2 --gradient_accumulation_steps 32 --dataloader_drop_last True --train_group_size 3 --max_len 512 --weight_decay 0.01 --logging_steps 10 --save_steps 100 ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1be5e43157904dd8839e73ce395c1750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bge_m3_ef = model.hybrid.BGEM3EmbeddingFunction(\n",
    "        model_name= \"BAAI/bge-m3\",\n",
    "        batch_size = 16,\n",
    "        device = \"cuda:0\",\n",
    "        # use_fp16 = True,\n",
    "        return_dense = True,\n",
    "        return_sparse = False,\n",
    "        return_colbert_vecs = False,\n",
    "    )\n",
    "\n",
    "client = MilvusClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MRR100(eval_code_path, test_qrels_path, inference_path):\n",
    "    command = [\"python\", eval_code_path, test_qrels_path, inference_path] \n",
    "    result = subprocess.run(command, capture_output=True, text=True)\n",
    "    stdout = result.stdout\n",
    "    stdout_value = float(stdout.split(\"\\n\")[1].split(\" \")[-1])\n",
    "    return stdout_value\n",
    "\n",
    "def inference(test_qid, model_name, output_path):\n",
    "\n",
    "    bge_rf = BGERerankFunction(\n",
    "        model_name=model_name,\n",
    "        device=\"cuda:0\" # Specify the device to use, e.g., 'cpu' or 'cuda:0'\n",
    "    )\n",
    "\n",
    "    result = []\n",
    "    error_list = []\n",
    "\n",
    "    for qid in tqdm(test_qid):\n",
    "        try:\n",
    "            query = data[data[\"qid\"] == qid][\"query\"].tolist()[0]\n",
    "\n",
    "            query_vectors = bge_m3_ef.encode_queries([query])[\"dense\"]\n",
    "\n",
    "            candidate = client.search(\n",
    "                collection_name=\"msmarco_bgem3\",  # target collection\n",
    "                data=query_vectors,  # query vectors\n",
    "                limit=100,  # number of returned entities\n",
    "                output_fields=[\"pid\",\"text\"],\n",
    "                anns_field=\"dense_vector\"\n",
    "            )\n",
    "            candidate_text = [i[\"entity\"][\"text\"] for i in candidate[0]]\n",
    "            candidate_pid = np.array([i[\"entity\"][\"pid\"] for i in candidate[0]])\n",
    "\n",
    "            top_k = bge_rf(\n",
    "                query=query,\n",
    "                documents=candidate_text,\n",
    "                top_k=100,\n",
    "            )\n",
    "            for n,i in enumerate(top_k):\n",
    "                result.append([qid, candidate_pid[i.index], n+1])\n",
    "        except:\n",
    "            error_list.append(qid)\n",
    "        break\n",
    "\n",
    "    result_df = pd.DataFrame(result)\n",
    "    result_df.to_csv(output_path, sep='\\t', index=False)\n",
    "\n",
    "    mrr_score = get_MRR100(\"../ms_marco_eval.py\", \"../data/test_qrels.tsv\", output_path)\n",
    "\n",
    "    model_name_item = model_name.split(\"/\")[-1]\n",
    "    print(f\"{model_name_item} : {mrr_score}\")\n",
    "    print(f\"error_list : {len(error_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m test_qid \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/test_qrels.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m, names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml\u001b[39m\u001b[38;5;124m'\u001b[39m])[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqid\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m----> 2\u001b[0m \u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_qid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBAAI/bge-reranker-v2-m3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../result/test.tsv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[30], line 10\u001b[0m, in \u001b[0;36minference\u001b[0;34m(test_qid, model_name, output_path)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minference\u001b[39m(test_qid, model_name, output_path):\n\u001b[0;32m---> 10\u001b[0m     bge_rf \u001b[38;5;241m=\u001b[39m \u001b[43mBGERerankFunction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Specify the device to use, e.g., 'cpu' or 'cuda:0'\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     result \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     16\u001b[0m     error_list \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/milvus_model/reranker/__init__.py:18\u001b[0m, in \u001b[0;36mBGERerankFunction\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mBGERerankFunction\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbgegreranker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBGERerankFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/milvus_model/reranker/bgereranker.py:31\u001b[0m, in \u001b[0;36mBGERerankFunction.__init__\u001b[0;34m(self, model_name, use_fp16, batch_size, normalize, device)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize \u001b[38;5;241m=\u001b[39m normalize\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m device\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreranker \u001b[38;5;241m=\u001b[39m \u001b[43m_FlagReranker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_fp16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/milvus_model/reranker/bgereranker.py:90\u001b[0m, in \u001b[0;36m_FlagReranker.__init__\u001b[0;34m(self, model_name_or_path, use_fp16, cache_dir, device)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_fp16:\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mhalf()\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/transformers/modeling_utils.py:2692\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2687\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2688\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2689\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2690\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2691\u001b[0m         )\n\u001b[0;32m-> 2692\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1173\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1170\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1171\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 779 (3 times)]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:804\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 804\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    807\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1159\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1154\u001b[0m             device,\n\u001b[1;32m   1155\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1156\u001b[0m             non_blocking,\n\u001b[1;32m   1157\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1158\u001b[0m         )\n\u001b[0;32m-> 1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "test_qid = pd.read_csv(\"../data/test_qrels.tsv\", sep='\\t', names=['qid', 'r', 'pid', 'l'])[\"qid\"].tolist()\n",
    "inference(test_qid, \"BAAI/bge-reranker-v2-m3\", \"../result/test.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
